{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fed590a-1a35-442f-9296-faeeff2f72be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "first we will create datasets\n",
    "- quotes_table\n",
    "- data enrichment table\n",
    "- postcodes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de390fd-7488-44c4-946f-740956a41eca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "catalog_name = \"lrcatalog\"\n",
    "schema_name = \"agentic_underwriting\"\n",
    "user_path = \"laurence.ryszka@databricks.com\"\n",
    "\n",
    "# Create catalog and schema\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba8d6f2-a4c0-40f6-9e6e-bf018b499a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#drop tables\n",
    "\n",
    "catalog_name = \"lrcatalog\"\n",
    "schema_name = \"agentic_underwriting\"\n",
    "\n",
    "# List of all known tables to drop\n",
    "tables_to_drop = [\n",
    "    \"quotes\",\n",
    "    \"claims_disclosure_validation\",\n",
    "    \"vehicle_risk_attributes\",\n",
    "    \"driver_risk_profile\",\n",
    "    \"property_attributes\",\n",
    "    \"agent_review\",\n",
    "    \"sales_call_transcripts\"\n",
    "\n",
    "]\n",
    "\n",
    "# Drop each table if it exists\n",
    "for table in tables_to_drop:\n",
    "    full_name = f\"{catalog_name}.{schema_name}.{table}\"\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {full_name}\")\n",
    "    print(f\"Dropped: {full_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df5e915a-3e8e-4b58-8704-fa1601baeb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create empty table for agent output\n",
    "spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {catalog_name}.{schema_name}.agent_review (\n",
    "  quote_id STRING,\n",
    "  agent_output STRING)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c7213f-5756-425a-a85c-27044ed5b95e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adjust this to match the actual path in your repo\n",
    "file_path = \"Repo/Users/laurence.ryszka@databricks.com/actuarial-pricing-demo/Agentic-Motor-underwriting/data/ONSPD_MAY_2025_UK_CR.csv\"  # relative path from notebook location\n",
    "\n",
    "# Load CSV using Spark\n",
    "df = spark.read.option(\"header\", True).csv(file_path)\n",
    "\n",
    "# Save to Unity Catalog\n",
    "table_name = \"uk_postcodes\"\n",
    "df.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c8b19a4-8043-414a-ba5a-97aa2b472c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "faker = Faker(\"en_GB\")  # Use UK-style names\n",
    "\n",
    "num_quotes = 15116 # don't change this as it must correspond to the number of postcodes in uk_postcodes table\n",
    "\n",
    "# Get list of real postcodes\n",
    "postcode_df = spark.table(f\"{catalog_name}.{schema_name}.uk_postcodes\").select(\"pcd\")\n",
    "postcode_list = [row.pcd for row in postcode_df.limit(num_quotes).collect()]\n",
    "\n",
    "# Reference lists\n",
    "vehicle_types = [\"Hatchback\", \"SUV\", \"Sedan\", \"Hot Hatch\", \"Van\"]\n",
    "storage_options = [\"garage\", \"driveway\", \"street\"]\n",
    "channels = [\"aggregator\", \"direct\"]\n",
    "\n",
    "# Generate synthetic quote rows\n",
    "quotes_data = []\n",
    "for i in range(len(postcode_list)):\n",
    "    first, last = faker.first_name(), faker.last_name()\n",
    "    age = random.randint(20, 75)\n",
    "    vehicle = random.choice(vehicle_types)\n",
    "    storage = random.choice(storage_options)\n",
    "    ncd = random.choice([0, 1, 3, 5, 10])\n",
    "    claims = random.choice([0, 1, 2])\n",
    "    pcd = postcode_list[i]\n",
    "    quotes_data.append({\n",
    "        \"quote_id\": f\"Q{1000+i}\",\n",
    "        \"first_name\": first,\n",
    "        \"last_name\": last,\n",
    "        \"age\": age,\n",
    "        \"postcode\": pcd,\n",
    "        \"vehicle_type\": vehicle,\n",
    "        \"storage_declared\": storage,\n",
    "        \"ncd_amount\": ncd,\n",
    "        \"claims_amount\": claims,\n",
    "        \"channel\": random.choice(channels)\n",
    "    })\n",
    "\n",
    "# Convert and save to Unity Catalog\n",
    "quotes_df = pd.DataFrame(quotes_data)\n",
    "quotes_sdf = spark.createDataFrame(quotes_df)\n",
    "quotes_sdf.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.quotes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d16e34b-233d-4241-bd67-f922436d19c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, round as round_col\n",
    "\n",
    "quotes_table = f\"{catalog_name}.{schema_name}.quotes\"\n",
    "\n",
    "# Load table\n",
    "quotes_df = spark.table(quotes_table)\n",
    "\n",
    "# Define multipliers as column expression\n",
    "vehicle_multiplier = when(col(\"vehicle_type\") == \"SUV\", 1.2) \\\n",
    "    .when(col(\"vehicle_type\") == \"Hatchback\", 1.0) \\\n",
    "    .when(col(\"vehicle_type\") == \"Sports\", 1.5) \\\n",
    "    .otherwise(1.1)\n",
    "\n",
    "# Define age adjustment\n",
    "age_adjustment = when(col(\"age\") < 25, 200) \\\n",
    "    .when(col(\"age\") < 35, 100) \\\n",
    "    .otherwise(0)\n",
    "\n",
    "# Define storage adjustment\n",
    "storage_adjustment = when(col(\"storage_declared\") == \"garage\", -50) \\\n",
    "    .when(col(\"storage_declared\") == \"driveway\", 0) \\\n",
    "    .when(col(\"storage_declared\") == \"street\", 50) \\\n",
    "    .otherwise(25)\n",
    "\n",
    "# Final price formula with all adjustments and multiplier\n",
    "base_price_expr = (\n",
    "    500 +\n",
    "    (col(\"claims_amount\") * 100) +\n",
    "    age_adjustment +\n",
    "    storage_adjustment -\n",
    "    (col(\"ncd_amount\") * 30)\n",
    ")\n",
    "\n",
    "# Apply vehicle multiplier and round\n",
    "priced_df = quotes_df.withColumn(\n",
    "    \"quote_value\",\n",
    "    round_col(base_price_expr * vehicle_multiplier, 2)\n",
    ")\n",
    "\n",
    "# Overwrite the table with updated prices\n",
    "priced_df.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").saveAsTable(quotes_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58bb4f05-5869-4be9-b27a-60e24ca82d75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "create enrichment tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff715fd8-8558-4e66-bf03-34c1fed70893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Read postcode list\n",
    "postcode_df = spark.table(f\"{catalog_name}.{schema_name}.uk_postcodes\").select(\"pcd\")\n",
    "postcode_list = [row.pcd for row in postcode_df.collect()]\n",
    "num_postcodes = len(postcode_list)\n",
    "\n",
    "# Generate driver_risk_profile\n",
    "ages = list(range(18, 101))  # 18 to 100 inclusive\n",
    "risk_segments = []\n",
    "\n",
    "for age in ages:\n",
    "    if age <= 21:\n",
    "        risk_segments.append(\"very_high_risk\")\n",
    "    elif age <= 30:\n",
    "        risk_segments.append(\"high_risk\")\n",
    "    elif age <= 60:\n",
    "        risk_segments.append(\"medium_risk\")\n",
    "    elif age <= 75:\n",
    "        risk_segments.append(\"high_risk\")\n",
    "    else:\n",
    "        risk_segments.append(\"very_high_risk\")\n",
    "\n",
    "# Build DataFrame\n",
    "driver_df = pd.DataFrame({\n",
    "    \"age\": ages,\n",
    "    \"risk_segment\": risk_segments\n",
    "})\n",
    "\n",
    "\n",
    "# Generate claims_disclosure_validation\n",
    "# Load quotes from Unity Catalog\n",
    "quotes_df = spark.table(\"lrcatalog.agentic_underwriting.quotes\").toPandas()\n",
    "\n",
    "# Create aligned claims_disclosure_validation data\n",
    "claims_data = []\n",
    "for _, row in quotes_df.iterrows():\n",
    "    declared_claims = row['claims_amount']\n",
    "    actual_claims = declared_claims + random.choice([-1, 0, 1])\n",
    "    claims_data.append({\n",
    "        \"first_name\": row[\"first_name\"],\n",
    "        \"last_name\": row[\"last_name\"],\n",
    "        \"postcode\": row[\"postcode\"],\n",
    "        \"ncd_amount\": random.choice([0, 1, 3, 5, 10]),\n",
    "        \"claims_amount\": max(0, actual_claims)\n",
    "    })\n",
    "\n",
    "# Convert and save to Unity Catalog\n",
    "claims_df = pd.DataFrame(claims_data)\n",
    "\n",
    "# Generate vehicle_risk_attributes (static)\n",
    "vehicle_types = [\"Hatchback\", \"SUV\", \"Sedan\", \"Hot Hatch\", \"Van\"]\n",
    "vehicle_risk_data = []\n",
    "for v in set(vehicle_types):\n",
    "    risk_band = random.choice([\"low\", \"medium\", \"high\"])\n",
    "    is_high_value = v in [\"Hot Hatch\", \"SUV\"] and risk_band == \"high\"\n",
    "    vehicle_risk_data.append({\n",
    "        \"vehicle_type\": v,\n",
    "        \"vehicle_risk_band\": risk_band,\n",
    "        \"is_high_value\": is_high_value\n",
    "    })\n",
    "vehicle_df = pd.DataFrame(vehicle_risk_data)\n",
    "\n",
    "# Generate property_attributes\n",
    "property_data = []\n",
    "risk_levels = [\"high\", \"mid_high\", \"mid_low\", \"low\"]\n",
    "num_levels = len(risk_levels)\n",
    "num_rows = len(postcode_list)\n",
    "\n",
    "# Assign risk levels descending over rows\n",
    "risk_per_block = num_rows // num_levels\n",
    "remaining = num_rows % num_levels\n",
    "\n",
    "# Build full list of risk levels in order\n",
    "ordered_risks = []\n",
    "for i in range(num_levels):\n",
    "    count = risk_per_block + (1 if i < remaining else 0)\n",
    "    ordered_risks.extend([risk_levels[i]] * count)\n",
    "\n",
    "for i, pc in enumerate(postcode_list):\n",
    "    property_data.append({\n",
    "        \"postcode\": pc,\n",
    "        \"has_garage\": random.choice([True, False]),\n",
    "        \"has_driveway\": random.choice([True, False]),\n",
    "        \"property_risk_level\": ordered_risks[i]\n",
    "    })\n",
    "\n",
    "property_df = pd.DataFrame(property_data)\n",
    "\n",
    "# Save all enrichment tables\n",
    "tables_to_save = {\n",
    "    \"driver_risk_profile\": driver_df,\n",
    "    \"claims_disclosure_validation\": claims_df,\n",
    "    \"vehicle_risk_attributes\": vehicle_df,\n",
    "    \"property_attributes\": property_df\n",
    "}\n",
    "\n",
    "for table_name, df in tables_to_save.items():\n",
    "    sdf = spark.createDataFrame(df)\n",
    "    sdf.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ece712f0-2808-420a-b879-2d825541b495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Add one quote we will base our demo on, must be added to quotes and claims disclosure validation table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d48578f3-1a8f-4cb3-87a8-15a0cc896f4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create call transcript table for sales calls\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Define catalog and schema\n",
    "table = \"sales_call_transcripts\"\n",
    "\n",
    "# Create transcript text\n",
    "transcript = \"\"\"Agent: Good morning, you're speaking with Amy from Swift Insurance. How can I help you today?\n",
    "\n",
    "Customer: Hi, I'd like to get a quote for my car insurance.\n",
    "\n",
    "Agent: Sure. Can I take your name, please?\n",
    "\n",
    "Customer: John Doe.\n",
    "\n",
    "Agent: Thanks, Mr. Doe. Can I confirm your postcode?\n",
    "\n",
    "Customer: CR3 6JF.\n",
    "\n",
    "Agent: And the type of vehicle?\n",
    "\n",
    "Customer: It's an SUV.\n",
    "\n",
    "Agent: Great. Where is the vehicle usually stored?\n",
    "\n",
    "Customer: On the driveway.\n",
    "\n",
    "Agent: Got it. And how many years of no claims discount do you have?\n",
    "\n",
    "Customer: 6 years.\n",
    "\n",
    "Agent: And how many claims have you had in the last 5 years?\n",
    "\n",
    "Customer: Three.\n",
    "\n",
    "Agent: Thank you. Just a moment while I generate your quote.\n",
    "\n",
    "...\n",
    "\n",
    "Agent: The quote I have for you today is £1332.\n",
    "\n",
    "Customer: Okay, thanks for your help.\"\"\"\n",
    "\n",
    "# Create DataFrame\n",
    "rows = [Row(quote_id=\"R9999\", call_transcript=transcript)]\n",
    "df = spark.createDataFrame(rows)\n",
    "\n",
    "# Save to Unity Catalog\n",
    "df.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffffce6a-eb4d-4420-8417-617fe5482e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Define catalog and schema\n",
    "table = \"sales_call_transcripts\"\n",
    "\n",
    "# Create transcript text for quote R9998\n",
    "transcript = \"\"\"Agent: Good afternoon, you're speaking with Olivia from Swift Insurance. How can I help today?\n",
    "\n",
    "Customer: Hi, I'm looking to get a quote for my car insurance.\n",
    "\n",
    "Agent: Of course. Can I take your name, please?\n",
    "\n",
    "Customer: Lauren Fish.\n",
    "\n",
    "Agent: Thank you, Ms. Fish. What's your postcode?\n",
    "\n",
    "Customer: CR3 6JF.\n",
    "\n",
    "Agent: Got it. What type of vehicle is it?\n",
    "\n",
    "Customer: It's a Hatchback.\n",
    "\n",
    "Agent: And where is the vehicle usually kept?\n",
    "\n",
    "Customer: On the driveway.\n",
    "\n",
    "Agent: Perfect. How many years of no claims discount do you have?\n",
    "\n",
    "Customer: 5 years.\n",
    "\n",
    "Agent: And any claims in the past 5 years?\n",
    "\n",
    "Customer: None.\n",
    "\n",
    "Agent: Thanks. Let me just generate your quote...\n",
    "\n",
    "...\n",
    "\n",
    "Agent: Alright, the quote I have for you is £450.\n",
    "\n",
    "Customer: That sounds good. Thank you!\"\"\"\n",
    "\n",
    "# Create DataFrame\n",
    "rows = [Row(quote_id=\"R9998\", call_transcript=transcript)]\n",
    "df = spark.createDataFrame(rows)\n",
    "\n",
    "# Save to Unity Catalog\n",
    "df.write.mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c221c24f-691b-4c1f-92da-6e7d73266621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "# First 'quotes' table\n",
    "# Define the new quote row\n",
    "new_quote = [Row(\n",
    "    quote_id=\"R9998\",\n",
    "    first_name=\"Lauren\",\n",
    "    last_name=\"Fish\",\n",
    "    age=61,\n",
    "    postcode=\"CR3 6JF\",\n",
    "    vehicle_type=\"Hatchback\",\n",
    "    storage_declared=\"driveway\",\n",
    "    ncd_amount=5,\n",
    "    claims_amount=0,\n",
    "    channel=\"direct\",\n",
    "    quote_value=float(450)\n",
    ")]\n",
    "\n",
    "# Convert to DataFrame\n",
    "new_quote_df = spark.createDataFrame(new_quote)\n",
    "\n",
    "# Append to existing quotes table\n",
    "new_quote_df.write.mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.quotes\")\n",
    "\n",
    "#now claims_disclosure_validation table\n",
    "# Define the new quote row\n",
    "new_val = [Row(\n",
    "    first_name=\"Lauren\",\n",
    "    last_name=\"Fish\",\n",
    "    postcode=\"CR3 6JF\",\n",
    "    ncd_amount=5,\n",
    "    claims_amount=0\n",
    ")]\n",
    "\n",
    "# Convert to DataFrame\n",
    "new_val_df = spark.createDataFrame(new_val)\n",
    "\n",
    "# Append to existing quotes table\n",
    "new_val_df.write.mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.claims_disclosure_validation\")\n",
    "\n",
    "#ensure our property_attributes table always has false for garage (random gen)\n",
    "# Create a single-row DataFrame with updated values\n",
    "updated_row = [Row(\n",
    "    postcode=\"CR3 6JF\",\n",
    "    has_garage=False,\n",
    "    has_driveway=True,\n",
    "    property_risk_level=\"mid_high\"\n",
    ")]\n",
    "\n",
    "updated_df = spark.createDataFrame(updated_row)\n",
    "\n",
    "# Overwrite existing row for the same postcode\n",
    "(\n",
    "    updated_df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .option(\"replaceWhere\", \"postcode = 'CR3 6JF'\")\n",
    "    .saveAsTable(f\"{catalog_name}.{schema_name}.property_attributes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd37694a-d5de-40f8-8cab-7e6f6781f078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "# First 'quotes' table\n",
    "# Define the new quote row\n",
    "new_quote = [Row(\n",
    "    quote_id=\"R9997\",\n",
    "    first_name=\"James\",\n",
    "    last_name=\"Bond\",\n",
    "    age=61,\n",
    "    postcode=\"CR3 6JE\",\n",
    "    vehicle_type=\"Hot Hatch\",\n",
    "    storage_declared=\"garage\",\n",
    "    ncd_amount=2,\n",
    "    claims_amount=0,\n",
    "    channel=\"direct\",\n",
    "    quote_value=float(450)\n",
    ")]\n",
    "\n",
    "# Convert to DataFrame\n",
    "new_quote_df = spark.createDataFrame(new_quote)\n",
    "\n",
    "# Append to existing quotes table\n",
    "new_quote_df.write.mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.quotes\")\n",
    "\n",
    "#now claims_disclosure_validation table\n",
    "# Define the new quote row\n",
    "new_val = [Row(\n",
    "    first_name=\"James\",\n",
    "    last_name=\"Bond\",\n",
    "    postcode=\"CR3 6JE\",\n",
    "    ncd_amount=2,\n",
    "    claims_amount=0\n",
    ")]\n",
    "\n",
    "# Convert to DataFrame\n",
    "new_val_df = spark.createDataFrame(new_val)\n",
    "\n",
    "# Append to existing quotes table\n",
    "new_val_df.write.mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.claims_disclosure_validation\")\n",
    "\n",
    "#ensure our property_attributes table always has false for garage (random gen)\n",
    "# Create a single-row DataFrame with updated values\n",
    "updated_row = [Row(\n",
    "    postcode=\"CR3 6JE\",\n",
    "    has_garage=False,\n",
    "    has_driveway=False,\n",
    "    property_risk_level=\"mid_high\"\n",
    ")]\n",
    "\n",
    "updated_df = spark.createDataFrame(updated_row)\n",
    "\n",
    "# Overwrite existing row for the same postcode\n",
    "(\n",
    "    updated_df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .option(\"replaceWhere\", \"postcode = 'CR3 6JE'\")\n",
    "    .saveAsTable(f\"{catalog_name}.{schema_name}.property_attributes\"))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ddad5e-6e2c-4a6c-9cbc-db0fbf3367e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "# First 'quotes' table\n",
    "# Define the new quote row\n",
    "new_quote = [Row(\n",
    "    quote_id=\"R9999\",\n",
    "    first_name=\"John\",\n",
    "    last_name=\"Doe\",\n",
    "    age=32,\n",
    "    postcode=\"CR3 6JF\",\n",
    "    vehicle_type=\"SUV\",\n",
    "    storage_declared=\"driveway\",\n",
    "    ncd_amount=3,\n",
    "    claims_amount=6,\n",
    "    channel=\"direct\",\n",
    "    quote_value=float(1332)\n",
    ")]\n",
    "\n",
    "# Convert to DataFrame\n",
    "new_quote_df = spark.createDataFrame(new_quote)\n",
    "\n",
    "# Append to existing quotes table\n",
    "new_quote_df.write.mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.quotes\")\n",
    "\n",
    "#now claims_disclosure_validation table\n",
    "# Define the new quote row\n",
    "new_val = [Row(\n",
    "    first_name=\"John\",\n",
    "    last_name=\"Doe\",\n",
    "    postcode=\"CR3 6JF\",\n",
    "    ncd_amount=6,\n",
    "    claims_amount=3\n",
    ")]\n",
    "\n",
    "# Convert to DataFrame\n",
    "new_val_df = spark.createDataFrame(new_val)\n",
    "\n",
    "# Append to existing quotes table\n",
    "new_val_df.write.mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.claims_disclosure_validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02fca012-1b9e-406f-bd56-b316bda5a2fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "scoring for inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64d0dae2-83ba-409a-93f7-31a795883baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#to do - add cars table (it is in the volume), assign risk score to each model, then generate quotes using real cars from that table. for now we keep the simple car/van/whatever."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_c5986e9d-37a9-4446-a259-103b92642bc0",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3397375242324085,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "1. Setup and data generation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
